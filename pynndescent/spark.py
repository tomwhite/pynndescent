from functools import partial
import itertools
import math
import numpy as np
import scipy.sparse

from sklearn.utils import check_random_state

from pynndescent import distances
from pynndescent.utils import *

INT32_MIN = np.iinfo(np.int32).min + 1
INT32_MAX = np.iinfo(np.int32).max - 1

# Chunking functions
# Could use Zarr/Zappy/Anndata for these

def read_zarr_chunk(arr, chunks, chunk_index):
    return arr[
           chunks[0] * chunk_index[0] : chunks[0] * (chunk_index[0] + 1),
           chunks[1] * chunk_index[1] : chunks[1] * (chunk_index[1] + 1),
           chunks[2] * chunk_index[2] : chunks[2] * (chunk_index[2] + 1),
           ]

def get_chunk_indices(shape, chunks):
    """
    Return all the indices (coordinates) for the chunks in a zarr array, even empty ones.
    """
    return [
        (i, j, k)
        for i in range(int(math.ceil(float(shape[0]) / chunks[0])))
        for j in range(int(math.ceil(float(shape[1]) / chunks[1])))
        for k in range(int(math.ceil(float(shape[2]) / chunks[2])))
    ]

def get_chunk_sizes(shape, chunks):
    def sizes(length, chunk_length):
        res = [chunk_length] * (length // chunk_length)
        if length % chunk_length != 0:
            res.append(length % chunk_length)
        return res

    return itertools.product(sizes(shape[0], chunks[0]), sizes(shape[1], chunks[1]), sizes(shape[2], chunks[2]))

def read_chunks(arr, chunks):
    shape = arr.shape
    func = partial(read_zarr_chunk, arr, chunks)
    chunk_indices = get_chunk_indices(shape, chunks)
    return func, chunk_indices

def to_local_rows(sc, arr, chunks):
    func, chunk_indices = read_chunks(arr, chunks)
    return [func(i) for i in chunk_indices]

# Distributed heap functions
# Distributed heaps can only be created and merged. Heap push is a local
# operation only.

def make_heap_rdd(sc, n_points, size, chunk_size):
    shape = (3, n_points, size)
    chunks = (shape[0], chunk_size, shape[2])
    chunk_sizes = list(get_chunk_sizes(shape, chunks))
    def make_heap_chunk(chunk_size):
        return make_heap(chunk_size[1], chunk_size[2])
    return sc.parallelize(chunk_sizes, len(chunk_sizes)).map(make_heap_chunk)

def merge_heap_pairs(heap_pair1, heap_pair2):
    heap1_new, heap1_old = heap_pair1
    heap2_new, heap2_old = heap_pair2
    return merge_heaps(heap1_new, heap2_new), merge_heaps(heap1_old, heap2_old)

def merge_heaps(heap1, heap2):
    heap = heap1.copy()
    # TODO: check heaps have the same size
    s = heap2.shape
    for row in range(s[1]):
        for ind in range(s[2]): # TODO: reverse to make more efficient?
            index = heap2[0, row, ind]
            weight = heap2[1, row, ind]
            flag = heap2[2, row, ind]
            heap_push(heap, row, weight, index, flag)
    return heap

# Sparse heap functions

def make_heap_sparse(n_points, size):
    # scipy.sparse only supports 2D matrices, so we have one for each of the
    # three positions in the first axis
    rows = set() # which rows have been created
    indices = scipy.sparse.lil_matrix((n_points, size))
    weights = scipy.sparse.lil_matrix((n_points, size))
    is_new = scipy.sparse.lil_matrix((n_points, size))
    return rows, indices, weights, is_new

def heap_push_sparse(heap, row, weight, index, flag):
    """Push a new element onto the heap. The heap stores potential neighbors
    for each data point. The ``row`` parameter determines which data point we
    are addressing, the ``weight`` determines the distance (for heap sorting),
    the ``index`` is the element to add, and the flag determines whether this
    is to be considered a new addition.

    Parameters
    ----------
    heap: ndarray generated by ``make_heap``
        The heap object to push into

    row: int
        Which actual heap within the heap object to push to

    weight: float
        The priority value of the element to push onto the heap

    index: int
        The actual value to be pushed

    flag: int
        Whether to flag the newly added element or not.

    Returns
    -------
    success: The number of new elements successfully pushed into the heap.
    """
    rows = heap[0]
    all_indices = heap[1]
    all_weights = heap[2]
    all_is_new = heap[3]

    if row not in rows:
        # initialize
        all_indices[row] = -1
        all_weights[row] = np.infty
        all_is_new[row] = 0
        rows.add(row)

    indices = all_indices.getrowview(row)
    weights = all_weights.getrowview(row)
    is_new = all_is_new.getrowview(row)

    if weight >= weights[0,0]:
        return 0

    # break if we already have this element.
    for i in range(indices.shape[1]):
        if index == indices[0,i]:
            return 0

    # insert val at position zero
    weights[0,0] = weight
    indices[0,0] = index
    is_new[0,0] = flag

    # descend the heap, swapping values until the max heap criterion is met
    i = 0
    while True:
        ic1 = 2 * i + 1
        ic2 = ic1 + 1

        if ic1 >= all_indices.shape[1]:
            break
        elif ic2 >= all_indices.shape[1]:
            if weights[0,ic1] > weight:
                i_swap = ic1
            else:
                break
        elif weights[0,ic1] >= weights[0,ic2]:
            if weight < weights[0,ic1]:
                i_swap = ic1
            else:
                break
        else:
            if weight < weights[0,ic2]:
                i_swap = ic2
            else:
                break

        weights[0,i] = weights[0,i_swap]
        indices[0,i] = indices[0,i_swap]
        is_new[0,i] = is_new[0,i_swap]

        i = i_swap

    weights[0,i] = weight
    indices[0,i] = index
    is_new[0,i] = flag

    return 1

def densify(heap):
    return np.stack([heap[i].toarray() for i in (1, 2, 3)])

def densify0(heap):
    return np.stack([heap[i].toarray() for i in (0, 1, 2)])

# def read_heap_chunks_sparse(heap, chunks):
#     shape = heap[1].shape
#     def func(chunk_index):
#         return densify0(tuple(heap[i][chunks[0] * chunk_index[0] : chunks[0] * (chunk_index[0] + 0)] for i in (1, 2, 3)))
#     chunk_indices = [
#         (i, j)
#         for i in range(int(math.ceil(float(shape[0]) / chunks[0])))
#         for j in range(int(math.ceil(float(shape[1]) / chunks[1])))
#     ]
#     return func, chunk_indices
#
# def merge_heaps_sparse(heap1_dense, heap2_sparse):
#     pass

# NNDescent algorithm

def get_rng_state(random_state):
    random_state = check_random_state(random_state)
    return random_state.randint(INT32_MIN, INT32_MAX, 3).astype(np.int64)

def init_current_graph(data, n_neighbors, rng_state):
    # This is just a copy from make_nn_descent -> nn_descent
    r = np.random.RandomState()

    dist = distances.named_distances['euclidean']

    current_graph = make_heap(data.shape[0], n_neighbors)
    # for each row i
    for i in range(data.shape[0]):
        # choose K rows from the whole matrix
        r.seed(i)
        indices = rejection_sample2(n_neighbors, data.shape[0], r)
        # and work out the dist from row i to each of the random K rows
        for j in range(indices.shape[0]):
            d = dist(data[i], data[indices[j]])
            heap_push(current_graph, i, d, indices[j], 1)
            heap_push(current_graph, indices[j], d, i, 1)

    return current_graph

def init_current_graph_rdd(sc, data, n_neighbors, rng_state):
    dist = distances.named_distances['euclidean']
    n_vertices = data.shape[0]
    chunk_size = 4
    current_graph_rdd = make_heap_rdd(sc, n_vertices, n_neighbors, chunk_size)
    current_graph_chunks = (3, chunk_size, n_neighbors) # 3 is first heap dimension

    def init_current_graph_for_each_part(index, iterator):
        r = np.random.RandomState()
        offset = index * chunk_size
        for current_graph_part in iterator:
            n_vertices_part = current_graph_part.shape[1]
            # Each part has its own heap for the current graph, which
            # are combined in the reduce stage.
            current_graph_local = make_heap(n_vertices, n_neighbors)
            current_graph_local_sparse = make_heap_sparse(n_vertices, n_neighbors)
            for i in range(n_vertices_part):
                iabs = i + offset
                r.seed(iabs)
                indices = rejection_sample2(n_neighbors, n_vertices, r)
                for j in range(indices.shape[0]):
                    d = dist(data[iabs], data[indices[j]])
                    heap_push(current_graph_local, iabs, d, indices[j], 1)
                    heap_push(current_graph_local, indices[j], d, iabs, 1)
                    heap_push_sparse(current_graph_local_sparse, iabs, d, indices[j], 1)
                    heap_push_sparse(current_graph_local_sparse, indices[j], d, iabs, 1)

            # Split current_graph into chunks and return each chunk keyed by its index.

            # TODO: don't densify, since some rows are unitialized. Instead, merge into a dense heap
            current_graph_local_from_sparse = densify(current_graph_local_sparse)
            print("current_graph_local", current_graph_local)
            print("current_graph_local_from_sparse", current_graph_local_from_sparse)
            print("equal?", current_graph_local == current_graph_local_from_sparse)
            read_chunk_func_new, chunk_indices = read_chunks(current_graph_local, current_graph_chunks)
            for i, chunk_index in enumerate(chunk_indices):
                yield i, read_chunk_func_new(chunk_index)

    return current_graph_rdd \
        .mapPartitionsWithIndex(init_current_graph_for_each_part) \
        .reduceByKey(merge_heaps) \
        .values()

def build_candidates_rdd(sc, current_graph_rdd, n_vertices, n_neighbors, max_candidates,
                     rng_state, rho=0.5):
    chunk_size = 4
    candidate_chunks = (3, chunk_size, max_candidates) # 3 is first heap dimension

    def build_candidates_for_each_part(index, iterator):
        offset = index * chunk_size
        for current_graph_part in iterator:
            n_vertices_part = current_graph_part.shape[1]
            # Each part has its own heaps for old and new candidates, which
            # are combined in the reduce stage.
            # (TODO: make these sparse - use COO (or maybe LIL) to construct, then convert to CSR to slice)
            new_candidate_neighbors = make_heap(n_vertices, max_candidates)
            old_candidate_neighbors = make_heap(n_vertices, max_candidates)
            for i in range(n_vertices_part):
                iabs = i + offset
                r = np.random.RandomState()
                r.seed(iabs)
                for j in range(n_neighbors):
                    if current_graph_part[0, i, j] < 0:
                        continue
                    idx = current_graph_part[0, i, j]
                    isn = current_graph_part[2, i, j]
                    d = r.random_sample()
                    if r.random_sample() < rho:
                        c = 0
                        if isn:
                            c += heap_push(new_candidate_neighbors, iabs, d, idx, isn)
                            c += heap_push(new_candidate_neighbors, idx, d, iabs, isn)
                        else:
                            heap_push(old_candidate_neighbors, iabs, d, idx, isn)
                            heap_push(old_candidate_neighbors, idx, d, iabs, isn)

                        if c > 0 :
                            current_graph_part[2, i, j] = 0

            # Split candidate_neighbors into chunks and return each chunk keyed by its index.
            # New and old are the same size, so chunk indices are the same.
            read_chunk_func_new, chunk_indices = read_chunks(new_candidate_neighbors, candidate_chunks)
            read_chunk_func_old, chunk_indices = read_chunks(old_candidate_neighbors, candidate_chunks)
            for i, chunk_index in enumerate(chunk_indices):
                yield i, (read_chunk_func_new(chunk_index), read_chunk_func_old(chunk_index))

    candidate_neighbors_combined = current_graph_rdd\
        .mapPartitionsWithIndex(build_candidates_for_each_part)\
        .reduceByKey(merge_heap_pairs)\
        .values()

    return candidate_neighbors_combined


def build_candidates(sc, current_graph_rdd, n_vertices, n_neighbors, max_candidates,
                     rng_state, rho=0.5):
    candidate_neighbors_combined = build_candidates_rdd(sc, current_graph_rdd, n_vertices, n_neighbors, max_candidates, rng_state, rho).collect()

    # stack results (this should really be materialized to a store, e.g. as Zarr)
    new_candidate_neighbors_combined = np.hstack([pair[0] for pair in candidate_neighbors_combined])
    old_candidate_neighbors_combined = np.hstack([pair[1] for pair in candidate_neighbors_combined])

    return new_candidate_neighbors_combined, old_candidate_neighbors_combined

def nn_descent(sc, data, n_neighbors, rng_state, max_candidates=50,
               n_iters=10, delta=0.001, rho=0.5,
               rp_tree_init=False, leaf_array=None, verbose=False):

    dist = distances.named_distances['euclidean']

    n_vertices = data.shape[0]
    chunk_size = 4
    current_graph_chunks = (3, chunk_size, n_neighbors) # 3 is first heap dimension

    current_graph_rdd = init_current_graph_rdd(sc, data, n_neighbors, rng_state)

    for n in range(n_iters):

        candidate_neighbors_combined = build_candidates_rdd(sc, current_graph_rdd,
                                                     n_vertices,
                                                     n_neighbors,
                                                     max_candidates,
                                                     rng_state, rho)


        def nn_descent_for_each_part(index, iterator):
            offset = index * chunk_size
            for candidate_neighbors_combined_part in iterator:
                new_candidate_neighbors_part, old_candidate_neighbors_part = candidate_neighbors_combined_part
                n_vertices_part = new_candidate_neighbors_part.shape[1]
                # Each part has its own heaps for the current graph, which
                # are combined in the reduce stage.
                current_graph = make_heap(n_vertices, n_neighbors)
                c = 0 # not used yet (needs combining across all partitions)
                for i in range(n_vertices_part):
                    for j in range(max_candidates):
                        p = int(new_candidate_neighbors_part[0, i, j])
                        if p < 0:
                            continue
                        for k in range(j, max_candidates):
                            q = int(new_candidate_neighbors_part[0, i, k])
                            if q < 0:
                                continue

                            d = dist(data[p], data[q])
                            c += heap_push(current_graph, p, d, q, 1)
                            c += heap_push(current_graph, q, d, p, 1)

                        for k in range(max_candidates):
                            q = int(old_candidate_neighbors_part[0, i, k])
                            if q < 0:
                                continue

                            d = dist(data[p], data[q])
                            c += heap_push(current_graph, p, d, q, 1)
                            c += heap_push(current_graph, q, d, p, 1)

                # Split current_graph into chunks and return each chunk keyed by its index.
                read_chunk_func_new, chunk_indices = read_chunks(current_graph, current_graph_chunks)
                for i, chunk_index in enumerate(chunk_indices):
                    yield i, read_chunk_func_new(chunk_index)

        current_graph_rdd_updates = candidate_neighbors_combined\
            .mapPartitionsWithIndex(nn_descent_for_each_part)\
            .reduceByKey(merge_heaps)\
            .values()

        # merge the updates into the current graph
        current_graph_rdd = current_graph_rdd\
            .zip(current_graph_rdd_updates)\
            .map(lambda pair: merge_heaps(pair[0], pair[1]))

        # TODO: transfer c back from each partition and sum, in order to implement termination criterion
        # if c <= delta * n_neighbors * data.shape[0]:
        #     break

    # stack results (again, shouldn't collect result, but instead save to storage)
    current_graph = np.hstack(current_graph_rdd.collect())

    return deheap_sort(current_graph)